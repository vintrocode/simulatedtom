# Simulated Theory of Mind

Public repository for "Think Twice: Perspective-Taking Improves Large Language Modelsâ€™ Theory-of-Mind Capabilities".

## Example Commands

*Evaluate BigToM across all categories with GPT-3.5 with simulation method:*
```
$ python evaluate_bigtom.py --eval_model=gpt-3.5-turbo --all --method=simulation
```

*Evaluate BigToM on false belief action with llama2-13b with chain-of-thought method:*
```
$ python evaluate_bigtom.py --eval_model=meta-llama/Llama-2-13b-chat-hf --condition=false_belief --variable=action --method=cot
```

*Evaluate ToMi across all categories with GPT-4 with baseline method, while seeing inputs and outputs of model:*
```
$ python evaluate_tomi.py --eval_model=gpt-4 --method=baseline --verbose
```

*To log to wandb:*

```
$ python evaluate_tomi.py --eval_model=gpt-4 --method=baseline --wandb=1 --tags=gpt4baseline
```

## Requirements

You need the `transformers`, `openai`, and `mistralai` libraries to run most of the code.
You'll also need to specify API keys in a `.env` file (see `.env.template`).

To run local models, we use the HuggingFace pipeline.
Make sure to change the model cache directory -- currently, it is set to `CACHE_DIR = '/scratch/weights/llama2'` within `llm_utils.py`.

## Important key things:
If any accuracies seem abnormally high, please try running with the flag: `--gradeGPT`. Currently most grading is done by manual parsing, but sometimes this fails. For example, when evaluating chain-of-thought on BigToM with Llama2-7b, Llama tends to output both answer choices in its "chain of thought". Thus, PLEASE USE `--gradeGPT` for that specific scenario, e.g.
```
$ python evaluate_bigtom.py --eval_model=meta-llama/Llama-2-7b-chat-hf --all --gradeGPT
```

## Running Evaluations

There are two files used to run evaluations on BigToM and ToMi:

`evaluate_bigtom.py`

`evaluate_tomi.py`

---

### evaluate_bigtom.py



`--eval_model`: model to evaluate on. 
-  `gpt-4`
- `gpt-3.5-turbo`
- `mistral-tiny`
- `mistral-small`
- `mistral-medium`
- `meta-llama/Llama-2-7b-chat-hf`
- `meta-llama/Llama-2-13b-chat-hf`


`--method`: prompting method to evaluate with. 
- `baseline`
- `cot`
- `oneshot`
- `oneshotcot`
- `simulation`
- `onepromptsimulation` : This is the ablation (PT + sim in one prompt)


`--condition`: true or false belief.
- `true_belief`
- `false_belief`



`--variable`: action or belief.
- `belief`
- `action`


Other arguments:

- `--num_probs`: # of problems to evaluate. Default = 200.

- `--temperature`: Model temperature. Default = 0.0.

- `--wandb`: 1 for logging wandb, 0 for local run. Default = 0.

- `--tags`: Comma-separated tags for wandb.

- `--perspective_model`, `--sim_model` : If you want to use different models for perspective-taking and simulating. Default just uses `--eval_model` for both.

- `--gpu` : What GPU you want to run your model on. (Default = 0)


Boolean flags:

- `--all`: This will evaluate all four categories and ignore the `--condition` and `--variable` flags. 

- `--verbose`: Verbosity flag.

- `--gradeGPT`: This uses ChatGPT to grade the responses.

- `--eight_bit`: loads model in 8-bit.

---

### evaluate_tomi.py

The following flags are identical to above:

- `--eval_model`
- `--method`
    - `baseline`
    - `baselineRules`
    - `cot`
    - `cotRules`
    - `oneshot`
    - `oneshotcot`
    - `onePromptSimulation` (ablation)
    - `simulation`
- `--temperature`
- `--num_probs` (default=1000)
- `--wandb`, `--tags`
- `--perspective_model`, `--sim_model`
- `--gpu`
- `--verbose`

The difference for ToMi is the following: there are no `--condition`, `--variable`, `--all` flags, and instead:

- `--category` (Default = `"all"`): This is the question category you want to evaluate for ToMi. For example, `reality`, `first_order_0_no_tom`, etc...

## Repository Structure

`data/` : contains BigToM and ToMI datasets.
- BigToM has all generated data from the original BigToM paper, but we only use `0_forward` conditions.
- `data/fixedtomi/test_balanced.jsonl` contains 1000 ToMI questions, with 100 of each category, disambiguated and without the mislabelled second-order questions (courtesy of Sclar et al.)

`code/` : contains code for evaluations + simulation.
- `code/prompt_instructions` contains baseline prompts for BigToM, as in the original BigToM paper.
- All other prompts are in `prompts_bigtom.py` and `prompts_tomi.py`.
- `evaluate_bigtom.py` and `evaluate_tomi.py` are the evaluation scripts.
- `llm_utils.py` : Wrappers for ChatGPT and local models.
- `sim_utils_*.py` : Utilities for simulation for both datasets.
- `doHumanEvals.py` : Script for running ablation studies involving human perspectives or simulations. Data is in `human_eval_data`.

## Logging to wandb
Feel free to change the command line args of `--project` and `--entity` to log to your own wandb project.

Currently, the scripts log debugging content, including but not limited to the inputs and outputs of the model, in a wandb table, along with the total accuracy and accuracy across categories in wandb graphs.

The code is also logged to wandb for each run for posterity's sake.



